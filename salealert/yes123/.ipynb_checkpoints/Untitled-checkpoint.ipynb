{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import re\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "\n",
    "driverPath = \"chromedriver.exe\" #填入剛剛記下的路徑\n",
    "driver = webdriver.Chrome(driverPath) # Chrome\n",
    "driver.implicitly_wait(10)\n",
    "driver.get(\"https://www.yes123.com.tw/admin/index.asp\")\n",
    "search_input = driver.find_element_by_name(\"find_key1\")\n",
    "search_input.send_keys('富邦人壽')\n",
    "start_search_btn = driver.find_element_by_class_name(\"bt_search\")\n",
    "start_search_btn.click()\n",
    "condition = EC.visibility_of_element_located((By.CLASS_NAME,'sift'))\n",
    "WebDriverWait(driver, 20, 0.5).until(condition)\n",
    "soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "s = re.findall(\"第.+頁\",soup.text)[0]\n",
    "totalpage = int(re.findall('\\d+',s)[1])\n",
    "\n",
    "url_list=[]\n",
    "for page in range(1,totalpage+1):\n",
    "    print('page:'+str(page))\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    for i in soup.find_all(class_='jobname'):\n",
    "        if i.get('href'):\n",
    "            url_list.append('https://www.yes123.com.tw/admin/'+i['href'])\n",
    "    if page != totalpage :\n",
    "        #condition = EC.presence_of_element_located((By.CLASS_NAME,\"next\"))\n",
    "        condition = EC.element_to_be_clickable((By.CLASS_NAME,\"next\"))\n",
    "        WebDriverWait(driver, 20, 0.5).until(condition)\n",
    "        next_page = driver.find_element_by_class_name(\"next\")\n",
    "        next_page.click()\n",
    "\n",
    "result=[]\n",
    "for url in url_list:\n",
    "    print(url)\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    result.append([url,soup])\n",
    "    \n",
    "def getcontent(soup):\n",
    "    soup_ul = soup.find_all('ul')\n",
    "    sub_title = []\n",
    "    sub_content = []\n",
    "    for i in soup_ul:\n",
    "        tt = i.find_all(class_=\"tt\")\n",
    "        rr = i.find_all(class_=\"rr\")\n",
    "        if tt!=[] and rr!=[]:\n",
    "            for j in range(len(tt)):\n",
    "                sub_title.append(tt[j].text.replace(' ： ',''))\n",
    "                sub_content.append(rr[j].text.replace('\\n','').replace('\\t','').replace('\\xa0','').replace('每月薪資行情表我要申訴','').replace(' ','').replace('地圖','').replace('就業導航',''))\n",
    "    return dict(zip(sub_title,sub_content))\n",
    "\n",
    "colname = ['web','url','工作名稱','薪資','上班地點','上班時段','公司','工作性質','聯絡人']\n",
    "data=[]\n",
    "for row in result:\n",
    "    content = getcontent(row[1])\n",
    "    value = ['yes123',\n",
    "             row[0],\n",
    "             row[1].find_all('h1')[0].text,\n",
    "             content['薪資待遇'],\n",
    "             content['工作地點'],\n",
    "             content['上班時段'],\n",
    "             '富邦',\n",
    "             content['工作性質'],\n",
    "             content['連絡人'] ]\n",
    "    r = dict(zip(colname,value))\n",
    "    data.append(r)\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
